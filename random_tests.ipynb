{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import config\n",
    "from utils import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "            dbname=config.DATABASE['DBNAME'],\n",
    "            host=config.DATABASE['HOST'],\n",
    "            user=config.DATABASE['USER'],\n",
    "            password=config.DATABASE['PASSWORD'],\n",
    "            port=config.DATABASE['PORT'],\n",
    "        )\n",
    "\n",
    "engine = create_engine(\n",
    "            f\"postgresql://{config.DATABASE['USER']}:\"\n",
    "            f\"{config.DATABASE['PASSWORD']}@\"\n",
    "            f\"{config.DATABASE['HOST']}:\"\n",
    "            f\"{config.DATABASE['PORT']}/\"\n",
    "            f\"{config.DATABASE['DBNAME']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nall_ids = []\\nwith gzip.open(config.TWEETS_PATH, 'rb') as f:\\n    for row_number, current_tweet in enumerate(f):\\n        all_ids.append(json.loads(current_tweet.decode(encoding='utf-8'))['id'])\\n\\nprint(f'Unique id amount: {len(set(all_ids))}, out of total id amount: {len(all_ids)}')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "all_ids = []\n",
    "with gzip.open(config.TWEETS_PATH, 'rb') as f:\n",
    "    for row_number, current_tweet in enumerate(f):\n",
    "        all_ids.append(json.loads(current_tweet.decode(encoding='utf-8'))['id'])\n",
    "\n",
    "print(f'Unique id amount: {len(set(all_ids))}, out of total id amount: {len(all_ids)}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xschon/Desktop/PDT/Z1/utils/utilities.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100000\n",
    "\n",
    "data_rows = []\n",
    "conversations_existing_ids = utilities.run_written_query('SELECT id FROM conversations', to_dataframe=True, option='from_string').id.astype(str).values\n",
    "with gzip.open(config.TWEETS_PATH, 'rb') as f:\n",
    "    for row_number, current_tweet in enumerate(f):\n",
    "        data_rows.append(json.loads(current_tweet.decode(encoding='utf-8')))\n",
    "        if (row_number+1) % batch_size == 0:\n",
    "            print(row_number+1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_id', 'conversation_id', 'created_at', 'entities', 'id',\n",
       "       'language', 'possibly_sensitive', 'public_metrics', 'referenced_tweets',\n",
       "       'reply_settings', 'source', 'content', 'context_annotations',\n",
       "       'attachments', 'in_reply_to_user_id', 'geo', 'withheld', 'like_count',\n",
       "       'quote_count', 'reply_count', 'retweet_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.DataFrame(data_rows)\n",
    "metrics = pd.DataFrame(tweets.public_metrics.to_list())\n",
    "tweets[metrics.columns] = metrics\n",
    "tweets.rename(columns={'text' : 'content',\n",
    "                       'lang' : 'language',\n",
    "\n",
    "                    }, inplace=True)\n",
    "\n",
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO authors uploading when uploading conversations_table\n",
    "# TODO drop duplicates of ids\n",
    "\n",
    "conversations_table = tweets[['id', 'author_id', 'content', 'possibly_sensitive', 'language',\n",
    "        'source', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all referenced tweets - iterate over existing array of references for each tweet\n",
    "refs = tweets.dropna(subset='referenced_tweets')[['id', 'referenced_tweets']]\n",
    "conversation_references_table = pd.DataFrame(columns=['conversation_id', 'parent_id', 'type'])\n",
    "\n",
    "for layer in range(refs.referenced_tweets.apply(lambda x : len(x)).max()):\n",
    "    # Select all references from given layer and find tweets they reffer to\n",
    "    layer_references = refs.referenced_tweets.apply(lambda x : x[layer] if(len(x) > layer) else None).dropna()\n",
    "    conv_refs = pd.DataFrame(layer_references.to_list(), index=layer_references.index)\n",
    "    conv_refs.rename(columns={'id' : 'parent_id'}, inplace=True)\n",
    "    conv_refs = (conv_refs.join(refs.id).rename(columns={'id' : 'conversation_id'}))\n",
    "\n",
    "    conversation_references_table = pd.concat([conversation_references_table, conv_refs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_table = pd.DataFrame(columns=['conversation_id', 'expanded_url', 'title', 'description'])\n",
    "\n",
    "entities_index = tweets.dropna(subset='entities').entities.index\n",
    "links_raw = pd.DataFrame(tweets.dropna(subset='entities').entities.to_list(), index=entities_index).dropna(subset='urls').urls\n",
    "\n",
    "\n",
    "for layer in range(links_raw.apply(lambda x : len(x)).max()):\n",
    "    layer_links = links_raw.apply(lambda x : x[layer] if (len(x) > layer) else None).dropna()\n",
    "    tmp_links = pd.DataFrame(layer_links.to_list(), index=layer_links.index)\n",
    "    tmp_links = tmp_links.join(tweets.id).rename(columns={'id' : 'conversation_id'})\n",
    "\n",
    "    links_table = pd.concat([links_table, tmp_links])[list(links_table.columns)]\n",
    "\n",
    "# Delete links with longer than 255 char URL\n",
    "links_table = links_table[links_table.expanded_url.apply(lambda url_len : len(url_len) < 256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_table = pd.DataFrame(columns=['conversation_id', 'value', 'type', 'probability'])\n",
    "\n",
    "annotations_raw = pd.DataFrame(tweets.dropna(subset='entities').entities.to_list(), index=entities_index).dropna(subset='annotations').annotations\n",
    "\n",
    "for layer in range(annotations_raw.apply(lambda x : len(x)).max()):\n",
    "    layer_annotations = annotations_raw.apply(lambda x : x[layer] if (len(x)>layer) else None).dropna()\n",
    "    tmp_annotations  = pd.DataFrame(layer_annotations.to_list(), index=layer_annotations.index)\n",
    "    tmp_annotations.rename(columns={'normalized_text' : 'value'}, inplace=True)\n",
    "    tmp_annotations = tmp_annotations.join(tweets.id).rename(columns={'id' : 'conversation_id'})\n",
    "    \n",
    "    annotations_table = pd.concat([annotations_table, tmp_annotations])[list(annotations_table.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_raw = tweets.context_annotations.dropna()\n",
    "annotations_index = contexts_raw.index\n",
    "\n",
    "context_annotations_table= pd.DataFrame(columns=['conversation_id', 'context_domain_id', 'context_entity_id'])\n",
    "context_entities_table = pd.DataFrame(columns=['id', 'name', 'description'])\n",
    "context_domains_table = pd.DataFrame(columns=['id', 'name', 'description'])\n",
    "domain_entities_tmp = pd.DataFrame(columns=['domains', 'entities'])\n",
    "\n",
    "for layer in range(contexts_raw.apply(lambda x : len(x)).max()):\n",
    "    context_annotations = contexts_raw.apply(lambda x : x[layer] if (len(x) > layer) else None).dropna()\n",
    "    domain_entities_tmp = pd.DataFrame(context_annotations.to_list(), index=context_annotations.index)\n",
    "\n",
    "    ents = pd.DataFrame(domain_entities_tmp.entity.to_list(), index=context_annotations.index).join(tweets[['id']].rename(columns={'id' : 'conversation_id'}).conversation_id)\n",
    "    doms = pd.DataFrame(domain_entities_tmp.domain.to_list(), index=context_annotations.index).join(tweets[['id']].rename(columns={'id' : 'conversation_id'}).conversation_id)\n",
    "\n",
    "    context_entities_table = pd.concat([context_entities_table, ents])[context_entities_table.columns].drop_duplicates(subset='id')\n",
    "    context_domains_table = pd.concat([context_domains_table, doms])[context_domains_table.columns].drop_duplicates(subset='id')\n",
    "    \n",
    "    anns = ents[['id', 'conversation_id']].rename(columns={'id' : 'context_entity_id'}).join(doms[['id']].rename(columns={'id' : 'context_domain_id'}))\n",
    "    context_annotations_table = pd.concat([context_annotations_table, anns])[context_annotations_table.columns].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_full_list = pd.DataFrame(columns=['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_raw = pd.DataFrame(tweets.entities.dropna().to_list(), index=entities_index)\n",
    "hashtags = hashtags_raw.hashtags.dropna()\n",
    "all_hashtags = pd.DataFrame(columns=['tag'])\n",
    "\n",
    "for col in pd.DataFrame(hashtags.to_list()).columns:\n",
    "    all_hashtags = pd.concat([all_hashtags, pd.DataFrame(pd.DataFrame(hashtags.to_list())[col].to_list(), index=hashtags.index)])[['tag']]\n",
    "\n",
    "all_hashtags = all_hashtags.dropna()\n",
    "all_hashtags = all_hashtags.join(tweets.id).rename(columns={'id' : 'conversation_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_used_hashtags = all_hashtags[['tag']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hashtags = unique_used_hashtags[~unique_used_hashtags['tag'].isin(hashtags_full_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hashtags = unique_used_hashtags[~unique_used_hashtags['tag'].isin(hashtags_full_list)]\n",
    "hashtags_full_list = pd.concat([hashtags_full_list, new_hashtags], ignore_index=True)\n",
    "upload_hashtags = hashtags_full_list[hashtags_full_list.tag.isin(new_hashtags.tag)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
